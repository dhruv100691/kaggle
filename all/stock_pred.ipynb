{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n\n(market_train_df, news_train_df) = env.get_training_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "af00564c0e4caf26a858c2981761380133be3672"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.dates as mdates\nfig, ax= plt.subplots(1,2,figsize=(20, 5))\n#TODO: what is locator used for?\nyears = mdates.YearLocator()   # every year\nmonths = mdates.MonthLocator()  # every month\nyearsFmt = mdates.DateFormatter('%Y')\nmonthsFmt = mdates.DateFormatter('%Y-%m')\nmkt_apple = market_train_df[(market_train_df['assetCode'] == 'AAPL.O')& (market_train_df['time'] >= '2014-01-01')\n                                & (market_train_df['time'] <= '2015-01-01')] \n#ax.plot(mkt_apple['time'],mkt_apple['close'],label='close price')\nax[0].plot(mkt_apple['time'],mkt_apple['returnsClosePrevMktres10'],label='returnsClosePrevMktres10')\nax[0].plot(mkt_apple['time'],mkt_apple['returnsOpenPrevMktres10'],label='returnsOpenPrevMktres10')\nax[0].plot(mkt_apple['time'],mkt_apple['returnsOpenNextMktres10'],label='returnsOpenNextMktres10')\nax[0].xaxis.set_major_formatter(monthsFmt)\n#plt.setp(ax[0].get_xticklabels(), rotation=90)\nax[0].set_xlabel('Date')\nax[0].legend(loc='best')\nax[0].set_title('Apple Shares')\n\n\nax[1].plot(mkt_apple['time'],mkt_apple['volume'],label='volume')\nax[1].xaxis.set_major_formatter(monthsFmt)\n#plt.setp(ax.get_xticklabels(), rotation=90)\nax[1].set_xlabel('Date')\nax[1].legend(loc='best')\nax[1].set_title('Apple Shares Volume')\n\nmkt_apple['returnsOpenPrevMktres10'].describe()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "b31a42e3e1e835e1df8f0215ee9725a22b2543f8"
      },
      "cell_type": "code",
      "source": "from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\napple_news_sent = news_train_df[(news_train_df['assetCodes'].str.contains('AAPL.O',regex=False)) & (news_train_df['time'] >= '2014-01-01')\n                                & (news_train_df['time'] <= '2015-01-01')]\napple_news_sent['time']= pd.to_datetime(apple_news_sent['time']).dt.date\napple_news_sent = apple_news_sent.groupby(['time','sentimentClass']).count()['sourceTimestamp'].unstack()\nfig, ax= plt.subplots(1,2,figsize=(20, 5))\n#multiplying by 5 to enlarge the graphs,doesnt change the relative ratio. NS if we can do this\nax[0].bar(apple_news_sent.index,5*apple_news_sent[-1],label='Negative',color='red',bottom=None)\nax[0].bar(apple_news_sent.index,5*apple_news_sent[0],label='Neutral',color='blue',bottom=apple_news_sent[-1])\nax[0].bar(apple_news_sent.index,5*apple_news_sent[1],label='Positive',color='green',bottom=apple_news_sent[0])\nax[0].plot(mkt_apple['time'],mkt_apple['close'],label='close price')\nax[0].plot(mkt_apple['time'],mkt_apple['open'],label='open price')\nax[0].xaxis.set_major_formatter(monthsFmt)\nax[0].set_ylim(ymin=0,ymax=1000)\nax[0].legend(loc='best')\nax[0].set_title('Apple News')\nax[0].set_xlabel('Time')\n#ax.set_ylabel('Sentiment Levels')\nstop = set(stopwords.words('english'))\napple_news_sent = news_train_df[(news_train_df['assetCodes'].str.contains('AAPL.O',regex=False)) & (news_train_df['time'] >= '2014-01-01')\n                                & (news_train_df['time'] <= '2015-01-01')]\ntext = ' '.join(apple_news_sent['headline'].str.lower().values[-1000000:])\nwordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n                      width=1200, height=1000).generate(text)\n#plt.figure(figsize=(12, 8))\nax[1].imshow(wordcloud)\nax[1].set_title('Top words in headline')\n#ax[1].axis(\"off\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "19e354a5e9d436abb863781f608e66cc054b06d3"
      },
      "cell_type": "code",
      "source": "#code to test stationarity of apple share returns for next 10 days\nfrom statsmodels.tsa.stattools import adfuller\nfig,ax = plt.subplots(1,2,figsize=(20,5))\ndef test_stationarity(time_series,ax,tag):\n    #moving statistics averaged over last year, ~250 values per year\n    roll_mean = time_series.rolling(window=250).mean()\n    roll_std = time_series.rolling(window=250).std()\n    ax.plot(roll_mean,label='mean')\n    ax.plot(roll_std,label='std')\n    ax.plot(time_series,label=tag)\n    ax.xaxis.set_major_formatter(monthsFmt)\n    ax.legend(loc='best')\n    ax.set_title(tag)\n    ax.set_xlabel('Time')\n    #Perform Dickey-Fuller test to check stationarity:\n    #if test statistic less than critical value, null hypothesis can be rejected, and TS is stationary\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(time_series, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\nmkt_apple = market_train_df[(market_train_df['assetCode'] == 'AAPL.O')] \n#convert to time series by setting index as time,makes handling easier\nmkt_apple=mkt_apple.set_index('time')\n#to count entries by every year\n#mkt_apple['time'] = mkt_apple['time'].dt.strftime('%Y')\n#mkt_apple= mkt_apple.groupby('time').count()\ntest_stationarity(mkt_apple['returnsOpenNextMktres10'],ax[0],'returnsOpenNextMktres10')\ntest_stationarity(mkt_apple['open'],ax[1],'open price')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da7c05e1ecd94c65f11c4dc3f38c0d5acf9bd9b0"
      },
      "cell_type": "code",
      "source": "#making the open price stationary, by subtracting exponentially weighted moving avg\nmkt_apple = market_train_df[(market_train_df['assetCode'] == 'AAPL.O') ] \nmkt_apple=mkt_apple.set_index('time')\n#not transorfimng here, since it doesnt affect results\nts_log = np.log(mkt_apple['open'])\n#ts_log = mkt_apple['open']\nexpwighted_avg = ts_log.ewm(halflife=12).mean()\nfig,ax= plt.subplots(1,2,figsize=(20,5))\nax[0].plot(ts_log,label='open price')\nax[0].plot(expwighted_avg,label='EWMA')\nax[0].legend(loc='best')\nax[0].set_title('open price')\nax[0].set_xlabel('Time')\nts_log_ewma_diff = ts_log - expwighted_avg\ntest_stationarity(ts_log_ewma_diff,ax[1],'diff of open and ewma')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "91bde240e3a37c3f7e792f904b2709d3842a2560"
      },
      "cell_type": "code",
      "source": "mkt_apple_shift = ts_log - ts_log.shift()\nfig,ax= plt.subplots(1,1,figsize=(20,5))\nmkt_apple_shift.dropna(inplace=True)\ntest_stationarity(mkt_apple_shift,ax,'stationarity with differencing')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5620e84f39b655d50fcc8008773f642858d7dc3"
      },
      "cell_type": "code",
      "source": "#applying the ARIMA model, first determine p and q values using the autocorrelation and partial autocorrelation function\nfrom statsmodels.tsa.stattools import acf, pacf\nlag_acf = acf(mkt_apple_shift, nlags=20)\nlag_pacf = pacf(mkt_apple_shift, nlags=20, method='ols')\nfig,ax = plt.subplots(1,2,figsize=(20,5))\nax[0].plot(lag_acf)\nax[0].axhline(y=0,linestyle='--',color='gray')\nax[0].axhline(y=-1.96/np.sqrt(len(mkt_apple_shift)),linestyle='--',color='gray')\nax[0].axhline(y=1.96/np.sqrt(len(mkt_apple_shift)),linestyle='--',color='gray')\nax[0].set_title('Autocorrelation Function')\nax[1].plot(lag_pacf)\nax[1].axhline(y=0,linestyle='--',color='gray')\nax[1].axhline(y=-1.96/np.sqrt(len(mkt_apple_shift)),linestyle='--',color='gray')\nax[1].axhline(y=1.96/np.sqrt(len(mkt_apple_shift)),linestyle='--',color='gray')\nax[1].set_title('Partial Autocorrelation Function')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "077b6f5087ea5dbeeb26076f6fc514c269d4884b"
      },
      "cell_type": "code",
      "source": "from statsmodels.tsa.arima_model import ARIMA\n#AR model\nfig,ax = plt.subplots(2,2,figsize=(20,10))\nmodel = ARIMA(ts_log, order=(1, 1, 0))  #(p,d,q)\nresults_AR = model.fit(disp=-1)  \nax[0][0].plot(mkt_apple_shift)\nax[0][0].plot(results_AR.fittedvalues, color='red')\nax[0][0].set_title('RSS: %.4f'% sum((results_AR.fittedvalues-mkt_apple_shift)**2))\n#MA model\nmodel = ARIMA(ts_log, order=(0, 1, 1))  \nresults_AR = model.fit(disp=-1)  \nax[0][1].plot(mkt_apple_shift)\nax[0][1].plot(results_AR.fittedvalues, color='red')\nax[0][1].set_title('RSS: %.4f'% sum((results_AR.fittedvalues-mkt_apple_shift)**2))\n#combined model\n#MA model\nmodel = ARIMA(ts_log, order=(1, 1, 1))  \nresults_AR = model.fit(disp=-1)  \nax[1][0].plot(mkt_apple_shift)\nax[1][0].plot(results_AR.fittedvalues, color='red')\nax[1][0].set_title('RSS: %.4f'% sum((results_AR.fittedvalues-mkt_apple_shift)**2))\n#taking it back to original case\npredictions_ARIMA_diff = pd.Series(results_AR.fittedvalues, copy=True)\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n#print (predictions_ARIMA_diff_cumsum.head())\npredictions_ARIMA_log = pd.Series(ts_log.ix[0], index=ts_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA_log.head()\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nax[1][1].plot(mkt_apple['open'])\nax[1][1].plot(predictions_ARIMA)\nax[1][1].set_title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-mkt_apple['open'])**2)/len(mkt_apple['open'])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42ad94bdb317bd3f9dcd71610c45c0e9c4eadcd1"
      },
      "cell_type": "code",
      "source": "#General Analysis of closing prices\nfig,ax = plt.subplots(1,1,figsize=(20,5))\nfor quant in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(quant)\n    ax.plot(price_df,label='quantile'+str(quant))\nax.legend(loc='best')\nax.set_title('closing price')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c8cfdf4dd28c451a5927cdd6da588cca6b6e5137"
      },
      "cell_type": "code",
      "source": "market_train_df.isna().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8223fd63ff6cc048259d33dc98d787c7165b17d8"
      },
      "cell_type": "code",
      "source": "market_train_df['price_diff'] = market_train_df['close'] -market_train_df['open']\ngrouped = market_train_df.groupby('time')['price_diff'].agg(['std','min'])\nfig,ax=plt.subplots(1,1,figsize=(20,5))\nax.plot(grouped['std'])\nax.set(ylabel=\"std\",title=\"SD Price change in a day\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41ee98039c981a03767037a7b2f7180568668488"
      },
      "cell_type": "code",
      "source": "#since there seem to be some outliers, we need to eliminate them\nmarket_train_df['close_to_open'] = market_train_df['close'] / market_train_df['open']\nmarket_train_df['asset_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['asset_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n#replace with mean values if change in price is greater than 100%\nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n    if np.abs(row['asset_mean_open'] - row['open']) > np.abs(row['asset_mean_close'] - row['close']):\n        market_train_df.loc[i,'open'] = row['asset_mean_open']\n    else:\n        market_train_df.loc[i,'close'] = row['asset_mean_close']\n        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['asset_mean_open'] - row['open']) > np.abs(row['asset_mean_close'] - row['close']):\n        market_train_df.loc[i,'open'] = row['asset_mean_open']\n    else:\n        market_train_df.loc[i,'close'] = row['asset_mean_close']\nmarket_train_df['price_diff'] = market_train_df['close'] -market_train_df['open']\ngrouped = market_train_df.groupby('time')['price_diff'].agg(['std','min'])\nfig,ax=plt.subplots(1,1,figsize=(20,5))\nax.plot(grouped['std'])\nax.set(ylabel=\"std\",title=\"SD of Price change in a day\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f6f95e6844568b4aba9fa7db927458fb08fb9bf"
      },
      "cell_type": "code",
      "source": "fig,ax = plt.subplots(1,1,figsize=(20,5))\nmarket_train_df_sampled=market_train_df.loc[market_train_df['time']>='2011-01-01']\nfor col in ['returnsClosePrevRaw1','returnsClosePrevMktres1','returnsClosePrevRaw10','returnsClosePrevMktres10','returnsOpenNextMktres10']:\n    grouped=market_train_df_sampled.groupby('time')[col].mean()\n    ax.plot(grouped,label=col)\nax.legend(loc='best')\nax.set_title('Trend of Mean Values')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bdc1998fe5cdb0bfb79618278d6391a792567181"
      },
      "cell_type": "code",
      "source": "asset_training_by_day = market_train_df.groupby('time')['assetCode'].nunique()\nfig, ax= plt.subplots(1,2,figsize=(20,5))\nax[0].plot(asset_training_by_day)\nax[0].set(ylabel=\"Unique Assets\",title=\"Unique Assets per year\")\nassets_by_vol = market_train_df.groupby('assetCode')['volume'].sum()\nassets_by_vol = assets_by_vol.sort_values(ascending=False)[0:10]\nax[1].pie(assets_by_vol.values,labels=assets_by_vol.index,autopct='%1.1f%%')\nax[1].set(title=\"Highest Trading volumes per asset\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3773bc0563727bebd38a909ae53f797846318e3b"
      },
      "cell_type": "code",
      "source": "#no growth, no decrease\nprint (len(market_train_df[market_train_df['returnsOpenNextMktres10'] == 0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cff85adc877d9481f420bd55f8e3e72e1d74aeef"
      },
      "cell_type": "code",
      "source": "#replacing NULL values in marketres columns with raw values\nnull_columns=market_train_df.columns[market_train_df.isnull().any()]\nprint(market_train_df[null_columns].isnull().sum())\ncolumn_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ncolumn_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_raw)):\n    market_train_df[column_market[i]] = market_train_df[column_market[i]].fillna(market_train_df[column_raw[i]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1789b261840aa566e9d532b8e269cf3bd9441c48"
      },
      "cell_type": "code",
      "source": "#plotting the range of return values\nfig,ax = plt.subplots(2,2,figsize=(20,5))\nfor i,col in enumerate(column_market):\n    ax[i//2][i%2].hist(market_train_df[col].sample(10000).values)\n    ax[i//2][i%2].set(title=col)\n#removing outliers from returns columns\ncolumn_return = column_market + column_raw \ncolumn_return.append('returnsOpenNextMktres10')\nfor col in column_return:\n    makrket_train_df=market_train_df.loc[(market_train_df[col]>=-2) & (market_train_df[col]<=2)]\n#Remove strange data: Here we remove data with unknown asset name or asset codes with strange behavior. \n#For more details, see here: https://www.kaggle.com/nareyko/market-return-estimation-and-bad-data-detection\nmarket_train_df = market_train_df[~market_train_df['assetCode'].isin(['PGN.N','EBRYY.OB'])]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f966038d7c8b1e3ab99df943ffbae93074a5bfe"
      },
      "cell_type": "code",
      "source": "# Function to remove outliers from news data\ndef remove_outliers(data_frame, column_list, low=0.02, high=0.98):\n    for column in column_list:\n        this_column = data_frame[column]\n        quant_df = this_column.quantile([low,high])\n        low_limit = quant_df[low]\n        high_limit = quant_df[high]\n        data_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n    return data_frame\ncolumns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence','noveltyCount12H',\\\n                  'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H',\\\n                  'volumeCounts3D','volumeCounts5D','volumeCounts7D']\nprint('Clipping news outliers ...')\nnews_train_df = remove_outliers(news_train_df, columns_outlier)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "151c825d4bbc605129bbf4c1836dc9b05ee5c0ef"
      },
      "cell_type": "code",
      "source": "news_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af30c3c1b473a73253d87ea657f0ebf270985ca2"
      },
      "cell_type": "code",
      "source": "columns_news = ['firstCreated','relevance','sentimentClass','sentimentNegative','sentimentNeutral',\n               'sentimentPositive','noveltyCount24H','noveltyCount7D','volumeCounts24H','volumeCounts7D','assetCodes','sourceTimestamp',\n               'assetName','audiences', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n               'sentenceCount', 'firstMentionSentence','time']\nasset_code_dict = {k: v for v, k in enumerate(market_train_df['assetCode'].unique())}\ndef data_prep(market_df,news_df):\n    market_df['date'] = market_df.time.dt.date\n    market_df.drop(['time','asset_mean_open','asset_mean_close'], axis=1, inplace=True)\n    \n    news_df = news_df[columns_news]\n    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    news_df['firstCreated'] = news_df.firstCreated.dt.date\n    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    news_df['len_audiences'] = news_train_df['audiences'].map(lambda x: len(eval(x)))\n    print(news_df.head())\n    kcol = ['firstCreated', 'assetCodes']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n    market_df = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n                            right_on=['firstCreated', 'assetCodes'])\n    del news_df\n    market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n    market_df = market_df.drop(columns = ['firstCreated','assetCodes','assetName']).fillna(0) \n    return market_df\nmarket_train_df = data_prep(market_train_df, news_train_df)\nmarket_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a696589dad5d92eadb859c6493804bbb3a178af6"
      },
      "cell_type": "code",
      "source": "num_columns = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', \n               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'close_to_open', 'sourceTimestamp', 'urgency', 'companyCount', 'takeSequence', 'bodySize', 'sentenceCount',\n               'relevance', 'sentimentClass', 'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n               'noveltyCount24H','noveltyCount7D','volumeCounts24H','volumeCounts7D','assetCodesLen', 'asset_sentiment_count', 'len_audiences']\ncat_columns = ['assetCodeT']\nfeature_columns = num_columns+cat_columns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\ndata_scaler = StandardScaler()\n#data_scaler = MinMaxScaler()\nmarket_train_df[num_columns] = data_scaler.fit_transform(market_train_df[num_columns])\nmarket_train_df = market_train_df.reset_index()\nmarket_train_df = market_train_df.drop(columns='index')\n# Random train-test split\ntrain_indices, val_indices = train_test_split(market_train_df.index.values,test_size=0.1, random_state=92)\n\ndef get_input(market_train, indices):\n    X = market_train.loc[indices, feature_columns].values\n    y = market_train.loc[indices,'returnsOpenNextMktres10'].map(lambda x: 0 if x<0 else 1).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'date']\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train_df, train_indices)\nX_val,y_val,r_val,u_val,d_val = get_input(market_train_df, val_indices)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20dfe206ae3eb77aff430ed8a56f89f869bfd162"
      },
      "cell_type": "code",
      "source": "#parameter search takes long time\nimport lightgbm as lgb\n'''\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\ntune_params = {'n_estimators': [200,500,1000,2500,5000],\n              'max_depth': sp_randint(4,12),\n              'colsample_bytree':sp_uniform(loc=0.8, scale=0.15),\n              'min_child_samples':sp_randint(60,120),\n              'subsample': sp_uniform(loc=0.75, scale=0.25),\n              'reg_lambda':[1e-3, 1e-2, 1e-1, 1]}\n\nfit_params = {'early_stopping_rounds':40,\n              'eval_metric': 'accuracy',\n              'eval_set': [(X_train, y_train), (X_val, y_val)],\n              'verbose': 20,\n              'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_power)]}\n              \nlgb_clf = lgb.LGBMClassifier(n_jobs=4, objective='binary',random_state=1)\ngs = RandomizedSearchCV(estimator=lgb_clf, \n                        param_distributions=tune_params, \n                        n_iter=40,\n                        scoring='f1',\n                        cv=5,\n                        refit=True,\n                        random_state=1,\n                        verbose=True)\n'''\ndef learning_rate_power(current_round):\n    base_learning_rate = 0.19000424246380565\n    min_learning_rate = 0.01\n    lr = base_learning_rate * np.power(0.995,current_round)\n    return max(lr, min_learning_rate)\n\nfit_params = {'early_stopping_rounds':40,\n              'eval_metric': 'accuracy',\n              'eval_set': [(X_train, y_train), (X_val, y_val)],\n              'verbose': 20,\n              'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_power)]}\n\nlgb_clf = lgb.LGBMClassifier(n_jobs=4,\n                             objective='multiclass',\n                            random_state=100)\nopt_params = {'n_estimators':500,\n              'boosting_type': 'dart',\n              'objective': 'binary',\n              'num_leaves':2452,\n              'min_child_samples':212,\n              'reg_lambda':0.01}\nlgb_clf.set_params(**opt_params)\nlgb_clf.fit(X_train, y_train,**fit_params)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b72304f70ead9e110c716cdddb9fef88d585849"
      },
      "cell_type": "code",
      "source": "features_imp = pd.DataFrame()\nfeatures_imp['features'] = list(feature_columns)[:]\nfeatures_imp['importance'] = lgb_clf.feature_importances_\nfeatures_imp = features_imp.sort_values(by='importance', ascending=False).reset_index()\n\ny_plot = -np.arange(15)\nplt.figure(figsize=(10,6))\nplt.barh(y_plot, features_imp.loc[:14,'importance'].values)\nplt.yticks(y_plot,(features_imp.loc[:14,'features']))\nplt.xlabel('Feature importance')\nplt.title('Features importance')\nplt.tight_layout()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d25042064e7fcca209eecad0d53ef61d0cfe459a"
      },
      "cell_type": "code",
      "source": "# This code is inspired from this kernel: https://www.kaggle.com/skooch/lgbm-w-random-split-2\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.externals.joblib import Parallel, delayed\nfrom sklearn.base import clone\n\nclfs = []\nfor i in range(20):\n    clf = lgb.LGBMClassifier(learning_rate=0.1, random_state=1200+i, silent=True,\n                             n_jobs=4, n_estimators=2500)\n    clf.set_params(**opt_params)\n    clfs.append(('lgbm%i'%i, clf))\n\ndef split_data(X, y, test_percentage=0.2, seed=None):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_percentage)\n    return X_train, y_train, X_test, y_test \n\ndef _parallel_fit_estimator(estimator, X, y, sample_weight=None, **fit_params):\n    \n    # randomly split the data so we have a test set for early stopping\n    X_train, y_train, X_test, y_test = split_data(X, y, seed=1992)\n    \n    # update the fit params with our new split\n    fit_params[\"eval_set\"] = [(X_train,y_train), (X_test,y_test)]\n    \n    # fit the estimator\n    if sample_weight is not None:\n        estimator.fit(X_train, y_train, sample_weight=sample_weight, **fit_params)\n    else:\n        estimator.fit(X_train, y_train, **fit_params)\n    return estimator\n\nclass VotingClassifierLGBM(VotingClassifier):\n    '''\n    This implements the fit method of the VotingClassifier propagating fit_params\n    '''\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        \n        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n            raise NotImplementedError('Multilabel and multi-output'\n                                      ' classification is not supported.')\n\n        if self.voting not in ('soft', 'hard'):\n            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n                             % self.voting)\n\n        if self.estimators is None or len(self.estimators) == 0:\n            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n                                 ' should be a list of (string, estimator)'\n                                 ' tuples')\n\n        if (self.weights is not None and\n                len(self.weights) != len(self.estimators)):\n            raise ValueError('Number of classifiers and weights must be equal'\n                             '; got %d weights, %d estimators'\n                             % (len(self.weights), len(self.estimators)))\n\n        if sample_weight is not None:\n            for name, step in self.estimators:\n                if not has_fit_parameter(step, 'sample_weight'):\n                    raise ValueError('Underlying estimator \\'%s\\' does not'\n                                     ' support sample weights.' % name)\n        names, clfs = zip(*self.estimators)\n        self._validate_names(names)\n\n        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n        if n_isnone == len(self.estimators):\n            raise ValueError('All estimators are None. At least one is '\n                             'required to be a classifier!')\n\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        self.estimators_ = []\n\n        transformed_y = self.le_.transform(y)\n\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n                                                 sample_weight=sample_weight, **fit_params)\n                for clf in clfs if clf is not None)\n\n        return self\n    \nvc = VotingClassifierLGBM(clfs, voting='soft')\nvc.fit(X_train, y_train, **fit_params)\nfilename = 'VotingClassifierLGBM.sav'\npickle.dump(vc, open(filename, 'wb'))\nvc = pickle.load(open(filename, 'rb'))\nvc.voting = 'soft'\npredicted_class = vc.predict(X_val)\npredicted_return = vc.predict_proba(X_val)\n#predicted_return = confidence_out(predicted_return)\npredicted_return = vc.predict_proba(X_val)[:,1]*2-1\npredicted_return = rescale(predicted_return, r_train)\n\nvc.voting = 'soft'\nglobal_accuracy_soft = accuracy_score(y_val, predicted_class)\nglobal_f1_soft = f1_score(y_val, predicted_class)\nprint('Accuracy score clfs: %f' % global_accuracy_soft)\nprint('F1 score clfs: %f' % global_f1_soft)\n\nr_val = r_val.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = predicted_return * r_val * u_val\ndata = {'day' : d_val, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint('Validation score', score_valid)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e3b3772572731cac9f0ffc9dc5b20d9c8f662df"
      },
      "cell_type": "code",
      "source": "days = env.get_prediction_days()\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 50 == 0:\n        print(n_days,end=' ')\n\n    t = time.time()\n    column_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n    column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n    market_obs_df['close_open_ratio'] = np.abs(market_obs_df['close']/market_obs_df['open'])\n    for i in range(len(column_raw)):\n        market_obs_df[column_market[i]] = market_obs_df[column_market[i]].fillna(market_obs_df[column_raw[i]])\n\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(asset_code_dict.keys())]\n    market_obs = data_prep(market_obs_df, news_obs_df)\n    market_obs[num_columns] = data_scaler.transform(market_obs[num_columns])\n    X_live = market_obs[feature_columns].values\n    prep_time += time.time() - t\n\n    t = time.time()\n    lp = vc.predict_proba(X_live)\n    prediction_time += time.time() -t\n\n    t = time.time()\n    confidence = lp[:,1] - lp[:,0]\n    #confidence = confidence_out(lp)\n    confidence = rescale(confidence, r_train)\n    preds = pd.DataFrame({'assetCode':market_obs['assetCode'],'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59c74f4bbda7aebe9b033d9a7e26b346b7b04cdb"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}